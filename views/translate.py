import streamlit as st
import time
from utils import openai_helper
try:
    from streamlit_mic_recorder import mic_recorder
except ImportError:
    mic_recorder = None
import config

def render():
    st.markdown("<div class='section-title'>ğŸ—£ï¸ AI í†µì—­ì‚¬</div>", unsafe_allow_html=True)
    st.caption("í•œêµ­ì–´ â†” ì¼ë³¸ì–´ ì‹¤ì‹œê°„ ë²ˆì—­")

    if not config.OPENAI_API_KEY:
        st.error("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    # ë ˆì´ì•„ì›ƒ
    col_opt, col_blank = st.columns([1, 2])
    with col_opt:
        direction = st.radio("ë²ˆì—­ ë°©í–¥", ["í•œêµ­ì–´ â†’ ì¼ë³¸ì–´", "ì¼ë³¸ì–´ â†’ í•œêµ­ì–´"], horizontal=True)

    source_lang = "Korean" if direction.startswith("í•œêµ­ì–´") else "Japanese"
    target_lang = "Japanese" if direction.startswith("í•œêµ­ì–´") else "Korean"

    tab_text, tab_photo = st.tabs(["ğŸ’¬ í…ìŠ¤íŠ¸/ìŒì„±", "ğŸ“· ì‚¬ì§„ ë²ˆì—­"])

    # --- í…ìŠ¤íŠ¸/ìŒì„± ---
    with tab_text:
        # ìŒì„± ì…ë ¥
        st.markdown("##### ğŸ™ï¸ ìŒì„± ì…ë ¥")
        if mic_recorder:
            col_mic, col_status = st.columns([1, 4])
            with col_mic:
                audio = mic_recorder(
                    start_prompt="â— ë…¹ìŒ",
                    stop_prompt="â–  ì •ì§€",
                    just_once=True,
                    key="mic_recorder",
                )
            
            if audio and audio.get("bytes"):
                # ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ì¸ì§€ í™•ì¸
                if audio["bytes"] != st.session_state.get("last_mic_audio"):
                    st.session_state["last_mic_audio"] = audio["bytes"]
                    st.audio(audio["bytes"], format="audio/wav")
                    
                    # ìë™ í…ìŠ¤íŠ¸ ë³€í™˜
                    lang_code = "ko" if source_lang == "Korean" else "ja"
                    with st.spinner("ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘..."):
                        transcript = openai_helper.transcribe_audio(
                            audio["bytes"], 
                            config.OPENAI_API_KEY, 
                            config.OPENAI_STT_MODEL, 
                            lang_code
                        )
                        st.session_state["source_text_input"] = transcript
                        st.rerun()

        st.divider()

        # í…ìŠ¤íŠ¸ ì…ë ¥ ë° ê²°ê³¼
        col1, col2 = st.columns(2)
        with col1:
             # ìœ„ì ¯ì´ st.session_state["source_text_input"] ê°’ì„ ê°€ì ¸ì˜´
            source_text = st.text_area("ì…ë ¥", height=150, key="source_text_input", placeholder="ë²ˆì—­í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.")
        with col2:
            st.text_area(
                "ê²°ê³¼", 
                height=150, 
                value=st.session_state.get("trans_result", ""),
                disabled=True
            )

        col_act1, col_act2 = st.columns([1, 3])
        with col_act1:
            if st.button("ë²ˆì—­í•˜ê¸°", type="primary", use_container_width=True):
                if source_text:
                    with st.spinner("ë²ˆì—­ ì¤‘..."):
                        res = openai_helper.translate_text(
                            source_text, source_lang, target_lang, 
                            config.OPENAI_API_KEY, config.OPENAI_TRANSLATE_MODEL
                        )
                        st.session_state["trans_result"] = res
                        st.rerun()

        with col_act2:
            if st.button("ğŸ”Š ê²°ê³¼ ë“£ê¸°"):
                target_text = st.session_state.get("trans_result", "")
                if target_text:
                    audio_data = openai_helper.text_to_speech(
                        target_text, config.OPENAI_API_KEY, 
                        config.OPENAI_TTS_MODEL, config.OPENAI_TTS_VOICE
                    )
                    # ì˜¤ë””ì˜¤ ë°ì´í„°(ë˜ëŠ” í…ìŠ¤íŠ¸)ì˜ í•´ì‹œê°’ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ ê°•ì œ ë¦¬ë Œë”ë§
                    # ëª¨ë°”ì¼ í˜¸í™˜ì„± ë° ì¬ìƒ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ í•„ìˆ˜
                    import hashlib
                    audio_hash = hashlib.md5(target_text.encode()).hexdigest()
                    st.audio(audio_data, format="audio/mp3", autoplay=True, key=f"tts_{audio_hash}")

    # --- ì‚¬ì§„ ---
    with tab_photo:
        img_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["png", "jpg", "jpeg"])
        if img_file:
            st.image(img_file, width=300)
            if st.button("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë²ˆì—­"):
                with st.spinner("ë¶„ì„ ì¤‘..."):
                    extracted = openai_helper.extract_text_from_image(
                        img_file.getvalue(), img_file.type, 
                        config.OPENAI_API_KEY, config.OPENAI_OCR_MODEL
                    )
                    if extracted:
                        translated = openai_helper.translate_text(
                            extracted, "Any", "Korean", # ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ í•­ìƒ í•œêµ­ì–´ë¡œ ë²ˆì—­
                            config.OPENAI_API_KEY, config.OPENAI_TRANSLATE_MODEL
                        )
                        st.session_state["ocr_extracted"] = extracted
                        st.session_state["ocr_translated"] = translated
                        st.rerun()
        
        if st.session_state.get("ocr_extracted"):
            c1, c2 = st.columns(2)
            c1.text_area("ì¶”ì¶œëœ í…ìŠ¤íŠ¸", st.session_state["ocr_extracted"], height=200)
            c2.text_area("ë²ˆì—­ ê²°ê³¼ (í•œêµ­ì–´)", st.session_state["ocr_translated"], height=200)
